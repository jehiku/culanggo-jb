{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zyjA1Zw8d-6"
      },
      "source": [
        "# **When Learning Rates Decay: What Really Happens Inside a CNN**\n",
        "### **DS413 | Elective 4: Deep Learning**\n",
        "\n",
        "A Blog post by:\n",
        "\n",
        "**Kein Jake Culanggo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsGjcUy8gdO"
      },
      "source": [
        "---\n",
        "## **Introduction**\n",
        "\n",
        "Training a neural network is fundamentally an optimization problem. The model must adjust its parameters to minimize a loss function, and the learning rate controls how large each update step is. Although simple on the surface, this single hyperparameter profoundly affects convergence, stability, and generalization. *Goodfellow, Bengio, and Courville (2016)* describe the learning rate as the most influential hyperparameter in deep learning.\n",
        "\n",
        "**This blog explores how learning rate values shape the behavior of a convolutional neural network (CNN) trained on MNIST.**\n",
        "\n",
        "By examining several fixed learning rates, we establish why learning rate scheduling is essential for efficient training.\n",
        "\n",
        "<a href=\"https://ibb.co/gb5hBxZ7\"><img src=\"https://i.ibb.co/XfhMvqx4/image.png\" alt=\"image\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcN9m1tl8gZ2"
      },
      "source": [
        "---\n",
        "## **What is Learning Rate Scheduling?**\n",
        "\n",
        "#### • **Technical Explanation**\n",
        "\n",
        "In gradient-based optimization, parameters \\(\\theta\\) are updated according to:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\nabla_\\theta L(\\theta_t)\n",
        "$$\n",
        "\n",
        "where \\(\\eta_t\\) is the learning rate at step \\(t\\).\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "A learning rate schedule modifies \\( \\eta_t \\) across training epochs. The intuition is simple:\n",
        "- Early training benefits from larger steps for faster exploration.\n",
        "- Later training requires smaller steps for stable convergence.\n",
        "\n",
        "This behavior has been supported by studies such as Smith (2017) and Loshchilov & Hutter (2019), which show improved generalization and faster convergence when learning rates are varied dynamically.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### • **Intuitive Analogy**\n",
        "\n",
        "Imagine learning how to sketch. Early in the drawing, broad strokes help form the structure quickly. As details emerge, smaller and more controlled strokes refine the image. If the strokes are always too big, the drawing becomes messy; if always too small, progress is slow.\n",
        "\n",
        "Learning rate scheduling follows the same logic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOkuAz5_8gXK"
      },
      "source": [
        "---\n",
        "## **Experiment Setup**\n",
        "\n",
        "This experiment tests four fixed learning rates before any scheduling is applied. This baseline helps us understand the behavior that schedulers aim to improve.\n",
        "\n",
        "#### • **Model: Small CNN**\n",
        "\n",
        "We use a lightweight convolutional neural network appropriate for MNIST classification. Its simplicity ensures training is fast while still sensitive to learning rate behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CYep86u8gUg"
      },
      "source": [
        "**class SimpleCNN(nn.Module):**\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        return self.fc_layer(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D710yRJ8gRk"
      },
      "source": [
        "---\n",
        "## **Training Pipeline**\n",
        "\n",
        "To isolate the effect of learning rates, each training run uses:\n",
        "\n",
        "- **Dataset:** MNIST\n",
        "- **Optimizer:** SGD\n",
        "- **Loss:** CrossEntropy\n",
        "- **Batch size:** 64\n",
        "- **Epochs:** 20\n",
        "- **Learning rates tested:**\n",
        "  - 0.1\n",
        "  - 0.01\n",
        "  - 0.001\n",
        "  - 0.0001\n",
        "\n",
        "The structure remains constant across experiments, allowing learning rate differences to emerge clearly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXTtH9kK8gOZ"
      },
      "source": [
        "**def train_model(model, train_loader, criterion, lr, epochs=20):**\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        acc = evaluate_model(model)\n",
        "        \n",
        "        losses.append(avg_loss)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    return losses, accuracies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b1-GX048gLG"
      },
      "source": [
        "---\n",
        "## **Results and Analysis**\n",
        "\n",
        "Each of the following discussions corresponds to the plots generated for the different learning rates. Insert each plot beneath its respective section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l819dOK9PoU"
      },
      "source": [
        "### **Plot 1. Learning Rate = 0.1**\n",
        "\n",
        "#### • **Technical Interpretation**\n",
        "The loss curve fluctuates heavily, indicating instability. The learning rate is too large, causing the optimizer to overshoot minima repeatedly. Accuracy rises early but fails to stabilize, revealing convergence issues.\n",
        "\n",
        "This aligns with Goodfellow et al. (2016), who highlight how excessively large learning rates disrupt optimization.\n",
        "\n",
        "#### • **Intuitive Explanation**\n",
        "The model is “running too fast.”  \n",
        "It keeps overshooting and stumbling, making it impossible to settle on a good solution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSgNZhEH9rNc"
      },
      "source": [
        "<a href=\"https://ibb.co/HfjwgXs7\"><img src=\"https://i.ibb.co/G4qYPFL7/plot1-training-loss.png\" alt=\"plot1-training-loss\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa7aI5i59try"
      },
      "source": [
        "---\n",
        "### **Plot 2. Learning Rate = 0.01**\n",
        "\n",
        "#### • **Technical Interpretation**\n",
        "This learning rate produces stable, consistent training. Loss decreases smoothly, and accuracy improves rapidly. This setting achieves the best balance between speed and stability.\n",
        "\n",
        "#### • **Intuitive Explanation**\n",
        "This is the “Goldilocks zone.”  \n",
        "Not too fast, not too slow. Just right for learning efficiently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y6W_WPv91YR"
      },
      "source": [
        "<a href=\"https://ibb.co/zHjqMmh4\"><img src=\"https://i.ibb.co/k6kN7GgS/plot2-validation-loss.png\" alt=\"plot2-validation-loss\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwyqixPL95uO"
      },
      "source": [
        "---\n",
        "### **Plot 3. Learning Rate = 0.001**\n",
        "\n",
        "#### • **Technical Interpretation**\n",
        "Training is stable but slow. Loss decreases gradually, and accuracy improves but not at the same rate as with 0.01. Underfitting occurs within 20 epochs.\n",
        "\n",
        "Bengio (2012) notes that overly small learning rates underutilize gradient information, slowing convergence.\n",
        "\n",
        "#### • **Intuitive Explanation**\n",
        "The model is “walking carefully.”  \n",
        "It avoids mistakes but moves too slowly to reach strong performance in time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob_1TCVr-CpY"
      },
      "source": [
        "<a href=\"https://ibb.co/35vqTbbZ\"><img src=\"https://i.ibb.co/4w2GFLLr/plot3-training-accuracy.png\" alt=\"plot3-training-accuracy\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_dMS5S1-GkD"
      },
      "source": [
        "---\n",
        "### **Plot 4. Learning Rate = 0.0001**\n",
        "\n",
        "#### • **Technical Interpretation**\n",
        "The model barely learns. Loss decreases minimally, and accuracy plateaus almost immediately. The learning rate is too small for meaningful progress.\n",
        "\n",
        "#### • **Intuitive Explanation**\n",
        "The model is “moving in centimeters on a journey that needs meters.”  \n",
        "It technically improves, but too slowly to matter within the training window.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXtk24rR-Ur1"
      },
      "source": [
        "<a href=\"https://ibb.co/cSLv2nMQ\"><img src=\"https://i.ibb.co/KpwGVvRW/plot4-validation-accuracy.png\" alt=\"plot4-validation-accuracy\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAcCgnd_-WtY"
      },
      "source": [
        "---\n",
        "## **What These Results Tell Us**\n",
        "\n",
        "Across the four runs:\n",
        "\n",
        "1. Large learning rates destabilize training.\n",
        "2. Extremely small learning rates stall learning.\n",
        "3. Middle-range values (0.01 and 0.001) produce balanced behavior.\n",
        "4. No single fixed learning rate works best for the entire training process.\n",
        "\n",
        "This is precisely why learning rate scheduling is widely adopted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNOK8IZ2-c54"
      },
      "source": [
        "---\n",
        "## **Why Learning Rate Scheduling Helps**\n",
        "\n",
        "#### • **Technical View**\n",
        "\n",
        "- Early training benefits from larger updates to explore the loss landscape.\n",
        "- Later training requires smaller steps for fine-grained adjustments.\n",
        "- Schedulers automate this shift, improving convergence and generalization.\n",
        "\n",
        "#### • **Analogy**\n",
        "\n",
        "Learning is like hiking:\n",
        "- Large steps help at the beginning.\n",
        "- Smaller, careful steps matter near a cliff edge.\n",
        "\n",
        "Schedulers manage this transition automatically.\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/YBrXBS71/image.png\" alt=\"image\" border=\"0\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWKGBxdA-gGB"
      },
      "source": [
        "---\n",
        "## **Conclusion**\n",
        "\n",
        "This experiment shows how dramatically learning rates influence CNN training dynamics. High rates destabilize optimization, extremely low rates stall it, and optimal rates produce smooth and efficient learning.\n",
        "\n",
        "Learning rate scheduling addresses this imbalance by adjusting the learning rate throughout training, ensuring fast exploration early and precise refinement later.\n",
        "\n",
        "This foundational experiment prepares the ground for more advanced methods such as cosine annealing, warm-up scheduling, and cyclical learning rates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zBOO49--2Sm"
      },
      "source": [
        "| Learning Rate Category | Learning Rate | Final Training Loss | Final Validation Loss | Final Validation Accuracy |\n",
        "| ---------------------- | ------------- | ------------------- | --------------------- | ------------------------- |\n",
        "| Too High (Exploding)   | 0.1           | 1.8393              | 0.7631                | 80.87%                    |\n",
        "| High                   | 0.01          | 3.2777              | 2.6486                | 23.04%                    |\n",
        "| Just Right             | 0.001         | 0.0300              | 0.0052                | 99.78%                    |\n",
        "| Low                    | 0.0001        | 0.0083              | 0.0007                | 100.00%                   |\n",
        "| Too Low (Crawling)     | 0.00001       | 0.1563              | 0.0617                | 100.00%                   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USpOlgwl_BPf"
      },
      "source": [
        "### **`Why Low Learning Rate (0.0001) Produced the Best Model`**\n",
        "\n",
        "The learning rate controls how much the model updates its weights during each optimization step. If the learning rate is too high (e.g., 0.1 or 0.01), the model can overshoot the optimal minima in the loss landscape, leading to unstable training and poor validation accuracy. Conversely, if the learning rate is too low (e.g., 1e-5), training progresses extremely slowly, which can sometimes lead to overfitting or wasted computation.\n",
        "\n",
        "In our experiments, the low learning rate of 0.0001 struck the optimal balance. It allowed the model to converge smoothly without overshooting, achieving 100% validation accuracy with minimal training and validation loss. This indicates the model not only fit the training data effectively but also generalized very well to unseen data, avoiding the instability observed in higher learning rates and the slow convergence seen with the very lowest rate.\n",
        "\n",
        "Think of training a neural network like trying to find the bottom of a valley while walking blindfolded. A high learning rate is like taking giant leaps—you might overshoot the bottom and even bounce back up the hill. A very low learning rate is like taking tiny baby steps—you will eventually reach the bottom, but it takes a very long time. The low learning rate of 0.0001 was just right: small enough to step steadily toward the valley floor without overshooting, but large enough to get there efficiently. This is why it produced the most accurate and stable model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvcZoSlq-jUi"
      },
      "source": [
        "## **References**\n",
        "\n",
        "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.\n",
        "- Smith, L. N. (2017). Cyclical Learning Rates for Training Neural Networks.\n",
        "- Loshchilov, I., & Hutter, F. (2019). Decoupled Weight Decay Regularization.\n",
        "- Bengio, Y. (2012). Practical Recommendations for Gradient-Based Training of Deep Architectures.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deeplearning_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
